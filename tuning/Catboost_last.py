# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ëª¨ë¸ ê´€ë ¨
from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedKFold, train_test_split

# í‰ê°€ ì§€í‘œ ë° ë©”íŠ¸ë¦­
from sklearn.metrics import (
    roc_auc_score, f1_score, precision_score, recall_score, accuracy_score,
    confusion_matrix, classification_report, precision_recall_curve, roc_curve
)

# ìµœì í™”
import optuna
from optuna.samplers import TPESampler

# ê¸°íƒ€
from collections import Counter
# -----------------------------
# 1. ê³ ì • ëœë¤ ì‹œë“œ ì„¤ì •
# -----------------------------
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# -----------------------------
# 2. ë°ì´í„° ë¡œë“œ ë° ê³ ì • validation set ë¶„ë¦¬
# -----------------------------
X_full = pd.read_csv("X_train.csv")
y_full = pd.read_csv("y_train.csv").squeeze()
X_test = pd.read_csv("X_test.csv")
y_test = pd.read_csv("y_test.csv").squeeze()

# 80% train, 20% val (ì¬í˜„ ê°€ëŠ¥í•˜ê²Œ stratified split)
X_train, X_val, y_train, y_val = train_test_split(
    X_full, y_full,
    test_size=0.2,
    stratify=y_full,
    random_state=RANDOM_STATE
)

# -----------------------------
# 3. Optuna ëª©ì  í•¨ìˆ˜ ì •ì˜
# -----------------------------
def objective(trial):
    params = {
        'iterations': trial.suggest_int('iterations', 300, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),
        'random_seed': RANDOM_STATE,
        'loss_function': 'Logloss',
        'eval_metric': 'AUC',
        'verbose': 0
    }

    aucs = []
    f1s = []

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

    for train_idx, val_idx in skf.split(X_train, y_train):
        X_tr, X_te = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_te = y_train.iloc[train_idx], y_train.iloc[val_idx]

        model = CatBoostClassifier(**params)
        model.fit(
            X_tr, y_tr,
            eval_set=(X_te, y_te),
            early_stopping_rounds=30,
            use_best_model=True
        )

        y_prob = model.predict_proba(X_te)[:, 1]
        y_pred = (y_prob >= 0.5).astype(int)

        aucs.append(roc_auc_score(y_te, y_prob))
        f1s.append(f1_score(y_te, y_pred, zero_division=0))

    return 1 * np.mean(aucs) + 0 * np.mean(f1s)


# -----------------------------
# 4. Optuna ìŠ¤í„°ë”” ì‹¤í–‰
# -----------------------------
study = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=RANDOM_STATE)
)
study.optimize(objective, n_trials=50)

# -----------------------------
# 5. ê³ ì • validation setì—ì„œ ìµœì¢… ì„±ëŠ¥ í‰ê°€
# -----------------------------
print("\nâœ… Best params:", study.best_params)
print("ğŸ“ˆ Best score (CV mean AUC+F1):", study.best_value)

model = CatBoostClassifier(**study.best_params)
model.fit(X_train, y_train, eval_set=(X_val, y_val),early_stopping_rounds=100, use_best_model=True)

# 4. Predict probabilities
y_pred_prob = model.predict_proba(X_test)[:, 1]

# 5. Optimal threshold by F1-score
precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_prob)
f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
f1s = f1s[:-1]  # thresholds ê¸¸ì´ì— ë§ì¶° ìë¥´ê¸°

# threshold ë²”ìœ„ í•„í„°ë§ (0.2~0.8)
valid_idx = np.where((thresholds >= 0.2) & (thresholds <= 0.8))[0]

# ê°€ì¥ ë†’ì€ F1-score ìœ„ì¹˜ ì°¾ê¸°
if len(valid_idx) > 0:
    best_idx = valid_idx[np.argmax(f1s[valid_idx])]
    th_f1_opt = thresholds[best_idx]
else:
    raise ValueError("âš ï¸ 0.2~0.8 ì‚¬ì´ì— ìœ íš¨í•œ thresholdê°€ ì—†ìŠµë‹ˆë‹¤.")  # fallback ì œê±°

# ìµœì¢… ì˜ˆì¸¡
y_pred_final = (y_pred_prob >= th_f1_opt).astype(int)
print(f"âœ… F1-score ê¸°ì¤€ ìµœì  threshold (0.2~0.8): {th_f1_opt:.4f}")
# 6. Evaluation
auc_score = roc_auc_score(y_test, y_pred_prob)
acc = accuracy_score(y_test, y_pred_final)
prec = precision_score(y_test, y_pred_final, zero_division=0)
rec = recall_score(y_test, y_pred_final, zero_division=0)
f1 = f1_score(y_test, y_pred_final, zero_division=0)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_final).ravel()
spec = tn / (tn + fp) if (tn + fp) > 0 else 0

print("\nEvaluation Metrics (F1-optimal threshold)")
print(f"AUC        : {auc_score:.4f}")
print(f"Accuracy   : {acc:.4f}")
print(f"Precision  : {prec:.4f}")
print(f"Recall     : {rec:.4f}")
print(f"Specificity: {spec:.4f}")
print(f"F1 Score   : {f1:.4f}\n")

print("Classification Report:")
print(classification_report(y_test, y_pred_final))

# 7. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.4f}")
plt.plot([0, 1], [0, 1], '--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 8. Confusion Matrix
cm = confusion_matrix(y_test, y_pred_final)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred 0', 'Pred 1'],
            yticklabels=['True 0', 'True 1'])
plt.title(f'Confusion Matrix (Threshold = {th_f1_opt:.4f})')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()


# ì˜ˆì¸¡ í™•ë¥  ì €ì¥
pd.DataFrame({'y_pred_prob': y_pred_prob}).to_csv("catboost_y_pred_prob.csv", index=False)
print("ğŸ“ ì˜ˆì¸¡ í™•ë¥  ì €ì¥ ì™„ë£Œ: catboost_y_pred_prob.csv")

# ìµœì¢… ì´ì§„ ì˜ˆì¸¡ê°’ ì €ì¥
pd.DataFrame({'y_pred_f1': y_pred_final}).to_csv("catboost_y_pred_f1.csv", index=False)
print("ğŸ“ ì´ì§„ ì˜ˆì¸¡ê°’ ì €ì¥ ì™„ë£Œ: catboost_y_pred_f1.csv")